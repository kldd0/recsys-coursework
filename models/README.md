# Модели

Директория содержит ML модель для генерации эмбеддингов (CLIP).

## Требования

Убедитесь, что установлены все зависимости из `requirements.txt` в корне проекта:

```bash
pip install -r requirements.txt
```

Ключевые зависимости для модели:
- `torch==2.6.0`
- `transformers==4.49.0`
- `pillow==11.1.0`
- `numpy>=1.24.3,<2.0.0` (совместимо с FAISS)

## Запуск

Модель загружается автоматически при запуске приложения:

```bash
streamlit run app.py
```

Модель инициализируется при первом использовании через кэш в `src/cache.py`.

### Пересоздание эмбеддингов

Если вы изменили модель или данные, пересоздайте векторный индекс:

```bash
python scripts/generate_embeddings.py
```

Этот скрипт:
1. Загрузит метаданные из `data/embeddings/metadata.pkl`
2. Сгенерирует эмбеддинги для всех товаров
3. Обновит FAISS индекс в `vector_store/faiss_index.bin`

## Использование модели

Модель `CLIPEmbeddingModel` предоставляет два метода:

- `encode_image(image: PIL.Image) -> np.ndarray` - генерация визуального эмбеддинга
- `encode_text(text: str) -> np.ndarray` - генерация текстового эмбеддинга

Эмбеддинги нормализованы (L2) и имеют размерность 512.

## Проблемы

Если модель не загружается:

1. Проверьте подключение к интернету (модель загружается с HuggingFace)
2. Убедитесь, что версии `torch` и `transformers` совместимы
3. Проверьте логи в `logs/app.log`
